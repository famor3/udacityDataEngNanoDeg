# Data Lake
---
  
## **Overview**
---
This project has us build an ETL pipeline using AWS. Data extraction from S3, process in Spark where data is transformed into dimensional tables. It is then loaded back into S3 for use by Sparkify's analytic team. Sparkify has grown their user and song database and would like to move their data warehouse to a data lake.

## **Project Datasets**
---
The datasets reside in AWS S3. Links for the datasets are as follows:

Song Data: s3://udacity-dend/song_data

Log Data:  s3://udacity-dend/log_data

## **Song Dataset**
---
The song dataset is a subset of [Million Song Dataset](http://millionsongdataset.com).
  
Sample record:  
```json
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

## **Log Dataset**
---
Logs dataset has been generated by [Event Simulator](https://github.com/Interana/eventsim).

Sample log file:
![Sample log file](log-data.png)

## Schema  
---  
### Fact Table  
**songplays** - record of data in log data associated with song plays for example, records with page `NextSong`

```
songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent
```

### Dimension Tables
**users** - table of app users

```
user_id, first_name, last_name, gender, level
```

**songs** - table of song titles in music db

```
song_id, title, artist_id, year, duration
```

**artists** - table of artists in music db

```
artist_id, name, location, latitude, longitude
```

**time** - table containing timestamps of songs in **songplays** by unit of time

```
start_time, hour, day, week, month, year, weekday
```

## Project Files
---
```dl.cfg``` -> configuration file for AWS
```etl.py``` -> load data from S3 into Spark then process data in Jupyter notebook


## Environment
---
Python 3.6 or above

AWS S3

AWS EMR

Apache Spark

JupyterLab 1.0 or above - for analysis

## To run
---
1. You should have an EMR cluster built in AWS similar to the following:
```aws emr create-cluster --name sparkify-emr-cluster --use-default-roles --release-label emr-6.0.0 --instance-count 3 --instance-type m5.xlarge --applications Name=JupyterHub Name=Spark Name=Hadoop Name=Hive Name=Livy --ec2-attributes KeyName=emr-cluster-linux,SubnetId=subnet-39076975 --log-uri s3://sparkify-lake```
2. Transfer the ```etl.py``` file to your S3 folder ```S3://sparkify-lake/```
3. After the EMR cluster spins up, SSH into it from aws cli as such: ```aws emr ssh --cluster-id [INSERT_CLUSTER_ID] --key-pair-file ~/.aws/[INSERT_KEYPAIR_FILE_NAME]```
4. At the EMR terminal, run the ```etl.py``` file as follows: ```spark-submit s3://sparkify-lake/etl.py```

#### References:
[AWS Documentation](https://docs.aws.amazon.com/index.html)

[Apache Spark Documentation](https://spark.apache.org/docs/latest/sql-getting-started.html)
