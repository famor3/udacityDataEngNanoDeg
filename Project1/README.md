# Data Modeling with Postgres
---
  
## **Overview**
---
This project has us apply Data Modeling techniques using PostgresSQL to build an ETL pipeline using Python. Sparkify, an internet startup, wants to analyze data they've collected on songs and user activity with their new music streaming app. Their team is collecting this data in JSON format for both users and songs. My role is to build a Postgres db designed to optimize queries on songplay analysis. I am to create a db schema and ETL pipeline to be used for analyzing their data.

## **Song Dataset**
---
The song dataset is a subset of [Million Song Dataset](http://millionsongdataset.com).
  
Sample record:  
```json
{"num_songs": 1, "artist_id": "ARD7TVE1187B99BFB1", "artist_latitude": null, "artist_longitude": null, "artist_location": "California - LA", "artist_name": "Casual", "song_id": "SOMZWCG12A8C13C480", "title": "I Didn't Mean To", "duration": 218.93179, "year": 0}
```

## **Log Dataset**
---
Logs dataset has been generated by [Event Simulator](https://github.com/Interana/eventsim).

Sample Record:
```json
{"artist":null,"auth":"Logged In","firstName":"Walter","gender":"M","itemInSession":0,"lastName":"Frye","length":null,"level":"free","location":"San Francisco-Oakland-Hayward, CA","method":"GET","page":"Home","registration":1540919166796.0,"sessionId":38,"song":null,"status":200,"ts":1541105830796,"userAgent":"\"Mozilla\/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/36.0.1985.143 Safari\/537.36\"","userId":"39"}
```

## Schema  
---  
### Fact Table  
**songplays** - record of data in log data associated with song plays for example, records with page `NextSong`

```
songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent
```

### Dimension Tables
**users** - table of app users

```
user_id, first_name, last_name, gender, level
```

**songs** - table of song titles in music db

```
song_id, title, artist_id, year, duration
```

**artists** - table of artists in music db

```
artist_id, name, location, latitude, longitude
```

**time** - table containing timestamps of songs in **songplays** by unit of time

```
start_time, hour, day, week, month, year, weekday
```

## Project Files
---
```test.ipynb``` -> Jupyter notebook to connect to Postgres and validate data loaded in TEST and PRODUCTION loads  
```create_tables.py``` -> used for creating **sparkify** database in Postgres and setup tables  
```etl.ipynb``` -> Jupyter notebook to analyze data in a TEST environment before loading in PRODUCTION  
```etl.py``` -> pipeline that reads and processes **song_data** and **log_data** into tables  
```sql_queries.py``` -> used to drop and create fact and dimension tables and insert queries  


## Environment
---
Python 3.6 or above

PostgresSQL 9.5 or above

psycopg2 2.7 or above - PostgresSQL database adapter for Python

JupyterLab 1.0 or above - for db analysis

## To run
---
Run the ```create_tables.py``` and the ```etl.py``` files independently as follows:
```
python create_tables.py
python etl.py
```

#### References:
[Psycopg](http://initd.org/psycopg/docs/)

[PostgreSQL Documentation](https://www.postgresql.org/docs/)

[Pandas Documentation](https://pandas.pydata.org/pandas-docs/stable/)
